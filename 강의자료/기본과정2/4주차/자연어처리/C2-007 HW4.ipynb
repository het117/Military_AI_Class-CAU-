{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mINKCFMOsi2"
      },
      "source": [
        "### 2-2. Assignment(week6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrxdAXsqwmZp"
      },
      "source": [
        "**[before]** charcater-level RNN\n",
        "\n",
        "appl → pple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHcMe4rfxOff"
      },
      "source": [
        "**[Assigment]** word-level RNN\n",
        "\n",
        "Repeat is the best medicine for → is the best medicine for memory\n",
        "\n",
        "- Hint: one-hot vector 대신 embedding 사용\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tExLBc2hwEHV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8-4AnkAGwHim"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Repeat', 'for', 'medicine', 'the', 'memory', 'best', 'is']\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Repeat is the best medicine for memory\".split()\n",
        "vocab = list(set(sentence))\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM9nAjMMyMDH",
        "outputId": "8eefbc2a-a444-43c6-f62e-26ace8107862"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: 'Repeat', 2: 'for', 3: 'medicine', 4: 'the', 5: 'memory', 6: 'best', 7: 'is', 0: '<unk>'}\n",
            "{'Repeat': 1, 'for': 2, 'medicine': 3, 'the': 4, 'memory': 5, 'best': 6, 'is': 7, '<unk>': 0}\n"
          ]
        }
      ],
      "source": [
        "word2index = {tkn: i for i, tkn in enumerate(vocab, 1)}\n",
        "word2index['<unk>']=0\n",
        "\n",
        "index2word = {value: key for key, value in word2index.items()}\n",
        "\n",
        "print(index2word)\n",
        "print(word2index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 7, 4, 6, 3, 2]])\n",
            "tensor([[7, 4, 6, 3, 2, 5]])\n"
          ]
        }
      ],
      "source": [
        "def build_data(sentence, word2index):\n",
        "    encoded = [word2index[token] for token in sentence] \n",
        "    input_seq, label_seq = encoded[:-1], encoded[1:] \n",
        "    input_seq = torch.LongTensor(input_seq).unsqueeze(0) \n",
        "    label_seq = torch.LongTensor(label_seq).unsqueeze(0) \n",
        "    \n",
        "    return input_seq, label_seq\n",
        "\n",
        "X, Y = build_data(sentence, word2index)\n",
        "\n",
        "print(X)\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=input_size)\n",
        "        self.rnn_layer = nn.RNN(input_size, hidden_size, batch_first=batch_first)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.size())\n",
        "        output = self.embedding_layer(x)\n",
        "        # print(output.size())\n",
        "        output, hidden = self.rnn_layer(output)\n",
        "        # print(output.size())\n",
        "        output = self.linear(output)\n",
        "        # print(output.size())\n",
        "        return output.view(-1, output.size(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = len(word2index)\n",
        "input_size = 5\n",
        "hidden_size = 20\n",
        "\n",
        "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<function <lambda> at 0x0000025B95486F70>\n",
            "[01/200] 2.1398 \n",
            "Repeat memory memory for for Repeat for\n",
            "\n",
            "[11/200] 2.0062 \n",
            "Repeat for memory for for for for\n",
            "\n",
            "[21/200] 1.8762 \n",
            "Repeat is memory for for for for\n",
            "\n",
            "[31/200] 1.7432 \n",
            "Repeat is the best for for memory\n",
            "\n",
            "[41/200] 1.6014 \n",
            "Repeat is the best for for memory\n",
            "\n",
            "[51/200] 1.4472 \n",
            "Repeat is the best for for memory\n",
            "\n",
            "[61/200] 1.2807 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[71/200] 1.1079 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[81/200] 0.9403 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[91/200] 0.7886 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[101/200] 0.6581 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[111/200] 0.5489 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[121/200] 0.4588 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[131/200] 0.3851 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[141/200] 0.3252 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[151/200] 0.2766 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[161/200] 0.2373 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[171/200] 0.2053 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[181/200] 0.1793 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[191/200] 0.1578 \n",
            "Repeat is the best medicine for memory\n",
            "\n"
          ]
        }
      ],
      "source": [
        "decode = lambda y: [index2word.get(x) for x in y]\n",
        "\n",
        "print(decode)\n",
        "\n",
        "for step in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X)\n",
        "    loss = loss_function(output, Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if step % 10 == 0:\n",
        "        print(\"[{:02d}/200] {:.4f} \".format(step + 1, loss))\n",
        "        pred = output.softmax(-1).argmax(-1).tolist()\n",
        "        print(\" \".join([\"Repeat\"] + decode(pred)))\n",
        "        print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('Project')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "b1bc945498f5240759fe663bf7e706a169be6ade5f25b8b3e8baba23720c06c5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
